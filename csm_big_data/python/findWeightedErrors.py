#!/bin/python
# encoding: utf-8
#================================================================================
#
#    findWeightedErrors.py
#
#    Â© Copyright IBM Corporation 2015-2018. All Rights Reserved
#
#    This program is licensed under the terms of the Eclipse Public License
#    v1.0 as published by the Eclipse Foundation and available at
#    http://www.eclipse.org/legal/epl-v10.html
#
#    U.S. Government Users Restricted Rights:  Use, duplication or disclosure
#    restricted by GSA ADP Schedule Contract with IBM Corp.
#
#================================================================================


import argparse
import json
import sets
import sys
import os

from datetime import datetime
from dateutil.parser import parse
from elasticsearch import Elasticsearch
from elasticsearch.serializer import JSONSerializer

import cast_helper as cast

TARGET_ENV='CAST_ELASTIC'

def build_mapping_query(es, body, ranges, error):

    (index, source, mapping, category) = (
        error.get("index", "_all"),
        error.get("source", None),
        error.get("mapping", []),
        error.get("category", None ))
    
    # TODO Error reporting. 
    if category is None:
        return (None, None)
    
    #  Adjust the source.
    if ranges and source:
        mm = ranges[1].get("multi_match")
        if mm :
            mm["fields"] = [source]
    
    # Set up some aggregations.
    fields=sets.Set([source])
    agg_filters={}
    should_mapping=[]

    # Iterate over the mapping
    for field_map  in mapping:

        # Parse the field mapping. 
        (field, value, boost, threshold) = (
            field_map.get("field", ""), 
            field_map.get("value", ""), 
            int(field_map.get("boost", 0)),
            field_map.get("threshold", None)
        )

        # Build the field query. 
        if threshold:
            should={
                "range" : { 
                    field : {
                        threshold : value,
                        "boost"   : boost
                    }
                }
            }
        elif value.find(" ") == -1:
            should={ 
                "regexp" : { 
                    field : {
                        "value" : value.lower(),
                        "boost" : boost
                    }
                } 
            }
        else:
            should={ 
                "match_phrase" : { 
                    field : {
                        "query" : value,
                        "boost" : boost
                    }
                } 
            }
        
        # Add to the aggregations.
        fields.add(field)

        agg_name="{0} | {1}".format(field,value)
        agg_filters[agg_name] = { "filter" : should }
        should_mapping.append(should)    
    
    # Build the body of the query. 
    body["query"]= {
        "bool" : {
            "must"   : ranges,
            "should" : should_mapping,
            "minimum_should_match" : 1
        }
    }
    
    body["_source"]+= list(fields)
    body["aggs"] = agg_filters

    # Perform the search operation.
    key_res = es.search(
        index=index,
        body=body
    )
    
    return (category, key_res)

def main(args):

    # Specify the arguments.
    parser = argparse.ArgumentParser(
        description='''A tool which takes a weighted listing of keyword searches and presents aggregations of this data to the user.''')
    
    parser.add_argument( '-a', '--allocationid', metavar='int', dest='allocation_id', default=-1,
        help='The allocation ID of the job.')
    parser.add_argument( '-j', '--jobid', metavar='int', dest='job_id', default=-1,
        help='The job ID of the job.')
    parser.add_argument( '-s', '--jobidsecondary', metavar='int', dest='job_id_secondary', default=0,
        help='The secondary job ID of the job (default : 0).')
    parser.add_argument( '-t', '--target', metavar='hostname:port', dest='target', default=None, 
        help='An Elasticsearch server to be queried. This defaults to the contents of environment variable "CAST_ELASTIC".')
    parser.add_argument( '-v', '--verbose', action='store_true',
        help='Displays the top --size logs matching the --errormap mappings.')
    parser.add_argument( '--size', metavar='size', dest='size', default=10,
        help='The number of results to be returned. (default=10)')
    parser.add_argument( '-H', '--hostnames', metavar='host', dest='hosts', nargs='*', default=None,
        help='A list of hostnames to filter the results to.')
    parser.add_argument( '--errormap', metavar="file", dest="err_map_file", default=None,
        help='A map of errors to scan the user jobs for, including weights.')

    args = parser.parse_args()

    # If the target wasn't specified check the environment for the target value, printing help on failure.
    if args.target is None:
        if TARGET_ENV in os.environ:
            args.target = os.environ[TARGET_ENV]
        else:
            parser.print_help()
            print("Missing target, '%s' was not set." % TARGET_ENV)
            return 2
    
    # Load the weighted error mapping.
    error_map=None
    if args.err_map_file:
        error_map = JSONSerializer().loads(open(args.err_map_file).read())

    if error_map is None:
        parser.print_help()
        print("Error map '%s', could not be loaded" % args.err_map_file)
        return 2
    
    # Open a connection to the elastic cluster, if this fails is wrong on the server.
    es = Elasticsearch(
        args.target, 
        sniff_on_start=True,
        sniff_on_connection_fail=True,
        sniffer_timeout=60
    )

    # Execute the query on the cast-allocation index.
    tr_res =  cast.search_job(es, args.allocation_id, args.job_id, args.job_id_secondary)

    total_hits = cast.deep_get(tr_res, "hits","total")

    if total_hits != 1:
        print("This implementation only supports queries where the hit count is equal to 1.")
        return 3

    # TODO make this code more fault tolerant
    hits = cast.deep_get(tr_res, "hits", "hits")
    tr_data = cast.deep_get(hits[0], "_source", "data")

    # ---------------------------------------------------------------------------------------------
    
    # Build the hostnames string:
    if args.hosts is None: 
        args.hosts = tr_data.get("compute_nodes")

    hostnames = {
        "multi_match" : {
            "query"                 : " ".join(args.hosts),
            "type"                  : "best_fields",
            "fields"                :  [ "hostname", "source"],
            "tie_breaker"           : 0.3,
            "minimum_should_match"  : 1
        }
    }

    # ---------------------------------------------------------------------------------------------


    (ranges, should_match) = cast.build_timestamp_range(
        tr_data.get("begin_time"), 
        cast.deep_get(tr_data, "history", "end_time"))
    
    ranges.append(hostnames)

    # ---------------------------------------------------------------------------------------------
    # Build a body for the mapping query.
    body={
        "_source" : [ "@timestamp"],
        "size"    : args.size,
    }

    # Check the keywords supplied by the json.
    results={}
    for error in error_map:
        (category, result) = build_mapping_query(es, body.copy(), ranges, error)
        results[category]=result


    print(" ")
    # Print the results.
    for category, response in sorted(results.iteritems(), 
            key=lambda (k, v): cast.deep_get( v, "hits", "max_score" ), reverse=True ):

        # Get aggregations. 
        aggregations = response.get("aggregations", [])
        total= cast.deep_get(response, "hits", "total")
        
        print( "\"{0}\" Max Score : {1}".format( category, 
            cast.deep_get(response, "hits", "max_score") ) ) 
        print( "\"{0}\" Count : {1}".format( category, total ) ) 
        
        # Sort aggregations by document count.
        for (aggregation,value) in sorted(aggregations.iteritems(), 
                key=lambda (k, v): v.get("doc_count"), reverse=True):
            print("  \"{0}\" : {1}".format( aggregation, value.get("doc_count") ) ) 

        if args.verbose:
            hits=cast.deep_get(response, "hits", "hits")

            print("\nTop {0} \"{1}\" Results:".format(len(hits), category))
            print("-"  * 42 )
            for hit in hits: 
                print(json.dumps(hit["_source"]))
        print("="  * 42 )
        print(" ")
    # ---------------------------------------------------------------------------------------------

if __name__ == "__main__":
    sys.exit(main(sys.argv))
