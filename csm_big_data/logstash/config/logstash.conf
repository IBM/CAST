input {

# Syslog
tcp {
	port => 10515
	type => "log-syslog"
}

# Generic JSON
tcp { 
    port => 10522
    codec => "json"
}

# Filebeats
beats {
    port => 10523
    codec=>"json"
}
} #end inputs


filter {

if [type] == "log-syslog" 
{
	grok {
		match => { "message" => "%{RSYSLOGDSV}" }
        overwrite => ["message"]
		patterns_dir => ["/etc/logstash/patterns/ibm_grok.conf"]
	} #end grok
	
	#force through date filter to drop microseconds and give us this timestamp yyyy-MM-dd'T'HH:mm:ss.SSSX
	date {
		match => ["timestamp","ISO8601"]
		target => "@timestamp"
	} #end date

    if [program_name] == "kernel"
    {
        # Attempt to derive the actual program name from the kernel message.
        grok { 
            match => {"message" => "%{KERNELMSG}" }
            overwrite => ["program_name"]
            patterns_dir => ["/etc/logstash/patterns/ibm_grok.conf"]
            tag_on_failure => [ ]
            add_tag => ["kernel"]
        }
    }
    else if [program_name] == "mmfs" {
        grok {
		    patterns_dir => ["/etc/logstash/patterns/ibm_grok.conf"]
            overwrite => ["message", "severity"]
            match => [ "message", "%{MMFSMSG}" ]
            add_tag => ["mmfs"]
        } 
    } # end mmfs
    else if [program_name] == "eventlog"
    {
    	grok {
            patterns_dir => ["/etc/logstash/patterns/mellanox_grok.conf"]
            overwrite => ["message", "severity","timestamp"]
    		match => [ "message", "%{MELLANOXMSG}" ]
    	} #end grok

    	date {
    		match => ["timestamp", "YYYY-MM-dd HH:mm:ss.SSS"]
    		target => "@timestamp"
    	}

        mutate { replace => {"type" => "log-mellanox-event"} }
    } #end mellanox_event_log
    
} #end syslog
else if [type] == "console"
{
	date {
		match => ["date","ISO8601"]
		target => "@timestamp"
	} #end date

    mutate {
        rename => { "node" => "hostname"}
        remove_field => ["date"]
        replace => { "type" =>"log-console"}
    } 
}
else if "transaction" in [tags] {
    mutate { remove_field => [ "beat", "host", "source", "offset", "prospector"] }
}
else if "ras" in [tags] and "csm" in [tags] {
    mutate{ 
        gsub => [ "time_stamp", ".(\d{3})\d{3}", ".\1" ]
        add_field => { "type" => "log-ras"  } 
    }

### Use logstash gsub filter plugin to process input data before sending to elastic index.
### For the two fields: data.begin_time and data.history.end_time, the blank is replaced
### with a letter T. Note: these fields have type "date" that requires the valid format.
### Also note: No change to csm_transaction.log with this process.

    mutate {
        gsub => [
           "[data][begin_time]", " ", "T",
           "[data][history][end_time]", " ", "T"
        ]
    }

    date {
    	match => ["time_stamp", "ISO8601","YYYY-MM-dd HH:mm:ss.SSS" ]
    	target => "time_stamp"
    }
}

# Successes are processed  through the event correlator failures are dropped into a failure log.
if "_grokparsefailure" not in [tags] and "_jsonparsefailure" not in [tags] {
    csm_event_correlator {
        events_dir => "/etc/logstash/patterns/events.yml" # The ras yaml pattern file.
        patterns_dir => "/etc/logstash/patterns/*.conf" # Patterns directory for grok patterns.
    }
}


} #end filters

output {
    if "transaction" not in [tags] {
        elasticsearch {
            hosts => [ _ELASTIC_IP_PORT_LIST_ ]
            index => "cast-%{type}-%{+YYYY.MM.dd}"
            http_compression =>true
            document_type => "_doc"
        }
    }
    else
    {
        elasticsearch {
            hosts => [ _ELASTIC_IP_PORT_LIST_ ]
            action => "update"
            index => "cast-%{type}"
            http_compression =>true
            doc_as_upsert => true
            document_id => "%{uid}"
            document_type => "_doc"
        }
    }

    if "_grokparsefailure" in [tags] or "_jsonparsefailure" in [tags] {
        file {
            path => "/var/log/logstash/logstash_parse_failure.log"
        }
    }

} #end outputs
